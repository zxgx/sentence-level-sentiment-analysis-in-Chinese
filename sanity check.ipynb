{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unk/embed = 4415/15107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:PyTorch version 1.3.0+cpu available.\n",
      "INFO:data_utils:Reading TRAINING set from data/hotel\\train.csv\n",
      "INFO:data_utils:Loading vocab & embedding from caches/hotel_tokenizer.pt\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "DEBUG:jieba:Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.688 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.688 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "INFO:data_utils:Read 3500 lines from data/hotel\\train.csv\n",
      "INFO:data_utils:Reading VALIDATION set from data/hotel\\val.csv\n",
      "INFO:data_utils:Read 500 lines from data/hotel\\val.csv\n",
      "INFO:data_utils:Reading TEST set from data/hotel\\test.csv\n",
      "INFO:data_utils:Read 800 lines from data/hotel\\test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 500 800\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from main import Config\n",
    "from data_utils import SentDatasetReader, BucketIterator\n",
    "\n",
    "config = Config(model_name='cnn', include_length=False)\n",
    "reader = SentDatasetReader(config)\n",
    "\n",
    "train_set, val_set, test_set = reader.train_set, reader.val_set, reader.test_set\n",
    "print(len(train_set), len(val_set), len(test_set))\n",
    "\n",
    "tokenizer = reader.tokenizer\n",
    "\n",
    "train_iter = BucketIterator(config, train_set[:50], tokenizer)\n",
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7392774224281311\n",
      "0.49718213081359863\n",
      "0.3517269492149353\n",
      "0.28869447112083435\n",
      "0.18163490295410156\n",
      "0.14854934811592102\n",
      "0.11790812760591507\n",
      "0.08855190873146057\n",
      "0.06789091229438782\n",
      "0.05303627625107765\n",
      "0.04093398153781891\n",
      "0.024127090349793434\n",
      "0.021641969680786133\n",
      "0.018726971000432968\n",
      "0.014116057194769382\n",
      "0.009684654884040356\n",
      "0.009819768369197845\n",
      "0.006484685465693474\n",
      "0.005935234483331442\n",
      "0.00454971706494689\n"
     ]
    }
   ],
   "source": [
    "from models import TextCNN\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "model = TextCNN(tokenizer.embedding, tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        text, label = batch['text'], batch['label']\n",
    "        pred = model(text)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        print(loss.item())\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:PyTorch version 1.3.0+cpu available.\n",
      "INFO:data_utils:Reading TRAINING set from data/hotel\\train.csv\n",
      "INFO:data_utils:Loading vocab & embedding from caches/hotel_tokenizer.pt\n",
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "DEBUG:jieba:Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.083 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.083 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "INFO:data_utils:Read 3500 lines from data/hotel\\train.csv\n",
      "INFO:data_utils:Reading VALIDATION set from data/hotel\\val.csv\n",
      "INFO:data_utils:Read 500 lines from data/hotel\\val.csv\n",
      "INFO:data_utils:Reading TEST set from data/hotel\\test.csv\n",
      "INFO:data_utils:Read 800 lines from data/hotel\\test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 500 800\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from main import Config\n",
    "from data_utils import SentDatasetReader, BucketIterator\n",
    "\n",
    "config = Config(model_name='rnn', include_length=True)\n",
    "reader = SentDatasetReader(config)\n",
    "\n",
    "train_set, val_set, test_set = reader.train_set, reader.val_set, reader.test_set\n",
    "print(len(train_set), len(val_set), len(test_set))\n",
    "\n",
    "tokenizer = reader.tokenizer\n",
    "\n",
    "train_iter = BucketIterator(config, train_set[:80], tokenizer)\n",
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6889774799346924\n",
      "0.7397022247314453\n",
      "0.6576491594314575\n",
      "0.7153794765472412\n",
      "0.688977062702179\n",
      "0.6684532761573792\n",
      "0.6300277709960938\n",
      "0.6727902293205261\n",
      "0.6633433103561401\n",
      "0.5792187452316284\n",
      "0.5504500865936279\n",
      "0.6030893325805664\n",
      "0.47300875186920166\n",
      "0.5201376080513\n",
      "0.34177902340888977\n",
      "0.4162200391292572\n",
      "0.37619826197624207\n",
      "0.30556660890579224\n",
      "0.28590646386146545\n",
      "0.10428847372531891\n",
      "0.3557618260383606\n",
      "0.07192455232143402\n",
      "0.055615682154893875\n",
      "0.449155330657959\n",
      "0.18784195184707642\n",
      "0.049717772752046585\n",
      "0.10942044109106064\n",
      "0.5472766757011414\n",
      "0.4109153747558594\n",
      "0.04546129330992699\n",
      "0.2228892743587494\n",
      "0.13001637160778046\n",
      "0.2464221566915512\n",
      "0.08700109273195267\n",
      "0.09485174715518951\n",
      "0.24580833315849304\n",
      "0.2056538462638855\n",
      "0.017403576523065567\n",
      "0.12352816760540009\n",
      "0.012450458481907845\n"
     ]
    }
   ],
   "source": [
    "from models import RNN\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "model = RNN(tokenizer.embedding, tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_iter:\n",
    "        text, length, label = batch['text'], batch['length'], batch['label']\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text, length)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        print(loss.item())\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_utils:Reading TRAINING set from data/hotel\\train.csv\n",
      "INFO:transformers.tokenization_utils:Model name 'hfl/chinese-bert-wwm-ext' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'hfl/chinese-bert-wwm-ext' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-bert-wwm-ext/vocab.txt from cache at C:\\Users\\lenovo\\.cache\\torch\\transformers\\246927bf3d8d74609ce2d43f6b7bb5e514479a0cc4749ad62bf03c91e7e2f405.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-bert-wwm-ext/added_tokens.json from cache at C:\\Users\\lenovo\\.cache\\torch\\transformers\\9e1406a8a3d37e6242ae5f609f67f6cb09591e7b67f0f1ab2353b59269403cec.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-bert-wwm-ext/special_tokens_map.json from cache at C:\\Users\\lenovo\\.cache\\torch\\transformers\\1a8de45a6d45fdeaee71281a32d80ff68b30ca797b899a24cdfc8abc44e4df39.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/hfl/chinese-bert-wwm-ext/tokenizer_config.json from cache at C:\\Users\\lenovo\\.cache\\torch\\transformers\\0bb9b10f12bbc78d9dd407203cf7e67b97a8cb6cf8dcc7fe3187ed65a9661fe8.1ade4e0ac224a06d83f2cb9821a6656b6b59974d6552e8c728f2657e4ba445d9\n",
      "INFO:data_utils:Read 3500 lines from data/hotel\\train.csv\n",
      "INFO:data_utils:Reading VALIDATION set from data/hotel\\val.csv\n",
      "INFO:data_utils:Read 500 lines from data/hotel\\val.csv\n",
      "INFO:data_utils:Reading TEST set from data/hotel\\test.csv\n",
      "INFO:data_utils:Read 800 lines from data/hotel\\test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 500 800\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 368 at dim 1 (got 366)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0e7c6525786c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrain_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Temp\\test\\data_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config, dataset, tokenizer, shuffle, sort)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Temp\\test\\data_utils.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m             \u001b[0mbatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Temp\\test\\data_utils.py\u001b[0m in \u001b[0;36mcreate_batch\u001b[1;34m(self, batch_data)\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         ret = {\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m             \u001b[1;34m'label'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         }\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 368 at dim 1 (got 366)"
     ]
    }
   ],
   "source": [
    "from main import Config\n",
    "from data_utils import SentDatasetReader, BucketIterator\n",
    "\n",
    "config = Config(model_name='bert', include_length=True, max_len=512)\n",
    "reader = SentDatasetReader(config)\n",
    "\n",
    "train_set, val_set, test_set = reader.train_set, reader.val_set, reader.test_set\n",
    "print(len(train_set), len(val_set), len(test_set))\n",
    "\n",
    "tokenizer = reader.tokenizer\n",
    "\n",
    "train_iter = BucketIterator(config, train_set[:50], tokenizer)\n",
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 3, 5, 7]\n",
    "print(x[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reboot]",
   "language": "python",
   "name": "conda-env-reboot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
